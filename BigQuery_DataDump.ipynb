{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset python_dump already exists\n",
      "Table python_dump_table already exists\n",
      "Table python_dump_table created successfully\n",
      "Index(['product_category_name', 'product_category_name_english'], dtype='object')\n",
      "Data from local CSV file dumped successfully to BigQuery table python_dump_table\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import LoadJobConfig\n",
    "from google.cloud.bigquery.job import SourceFormat\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = \"firsttry-425502\"\n",
    "DATASET_ID = \"python_dump\"\n",
    "TABLE_ID = \"python_dump_table\"\n",
    "\n",
    "# Set the path to the Google Cloud credentials JSON file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\\\Users\\\\jyoti\\\\Downloads\\\\firsttry-425502-c0fe51562a85.json\"\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create the dataset if it does not exist\n",
    "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
    "dataset.location = \"US\"\n",
    "\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{PROJECT_ID}.{DATASET_ID}\")  # Make an API request.\n",
    "    print(\"Dataset {} already exists\".format(DATASET_ID))\n",
    "except NotFound:\n",
    "    print(\"Dataset {} is not found\".format(DATASET_ID))\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "\n",
    "# Schema (replace with actual format of the table)\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"product_category_name\", \"string\", \"nullable\"),\n",
    "    bigquery.SchemaField(\"product_category_name_english\", \"string\", \"nullable\")\n",
    "]\n",
    "\n",
    "# Define the table object\n",
    "table_ref = dataset.table(TABLE_ID)\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "# Create the table if it does not exist\n",
    "try:\n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} already exists\".format(TABLE_ID))\n",
    "except NotFound:\n",
    "    print(\"Table {} is not found\".format(TABLE_ID))\n",
    "    table = client.create_table(table)\n",
    "\n",
    "print(\"Table {} created successfully\".format(TABLE_ID))\n",
    "\n",
    "# Load data from local CSV file\n",
    "\n",
    "data_path = \"C:\\\\Users\\\\jyoti\\\\OneDrive\\\\Desktop\\\\Kentron_Learning\\\\Olist_Ecomm_Data\\\\product_category_name_translation.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "print(data.columns)\n",
    "# Write data to BigQuery\n",
    "data = data.dropna()  # Drops rows with any null values\n",
    "\n",
    "# Write data to BigQuery\n",
    "job_config = LoadJobConfig(schema=schema, source_format=SourceFormat.CSV)\n",
    "job = client.load_table_from_dataframe(data, table_ref, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "print(\"Data from local CSV file dumped successfully to BigQuery table {}\".format(TABLE_ID))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raghw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
